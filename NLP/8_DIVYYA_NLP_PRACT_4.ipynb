{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_DIVYYA_NLP_PRACT_4.ipynb","provenance":[],"authorship_tag":"ABX9TyOJn8NAwjm8ccd3MIgvktVY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["4. Text Tokenization"],"metadata":{"id":"b2O8vZB1iR-J"}},{"cell_type":"markdown","source":["a. Tokenization using Pythonâ€™s split() function"],"metadata":{"id":"e4bavHz1iUru"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7rrnPwyiH72","executionInfo":{"status":"ok","timestamp":1646457894716,"user_tz":-330,"elapsed":22,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"4bb88b0b-a1ae-4947-c676-6d7b9fdcb13e"},"outputs":[{"output_type":"stream","name":"stdout","text":[" This tool is an a beta stage\n"," Alexa developers can use Get Metrics API to\n","seamlessly analyse metric\n"," It also supports custom skill model, prebuilt Flash Briefing\n","model, and the Smart Home Skill API\n"," You can use this tool for creation of monitors,\n","alarms, and dashboards that spotlight changes\n"," The release of these three tools will\n","enable developers to create visual rich skills for Alexa devices with screens\n"," Amazon\n","describes these tools as the collection of tech and tools for creating visually rich and\n","interactive voice experiences\n"," \n"]}],"source":["text = \"\"\" This tool is an a beta stage. Alexa developers can use Get Metrics API to\n","seamlessly analyse metric. It also supports custom skill model, prebuilt Flash Briefing\n","model, and the Smart Home Skill API. You can use this tool for creation of monitors,\n","alarms, and dashboards that spotlight changes. The release of these three tools will\n","enable developers to create visual rich skills for Alexa devices with screens. Amazon\n","describes these tools as the collection of tech and tools for creating visually rich and\n","interactive voice experiences. \"\"\"\n","data = text.split('.')\n","for i in data:\n","      print (i)"]},{"cell_type":"markdown","source":["b. Tokenization using Regular Expressions (RegEx)"],"metadata":{"id":"Yxt3fJm_ideK"}},{"cell_type":"code","source":["import nltk\n","# import RegexpTokenizer() method from nltk\n","from nltk.tokenize import RegexpTokenizer\n","# Create a reference variable for Class RegexpTokenizer\n","tk = RegexpTokenizer('\\s+', gaps = True)\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","# Use tokenize method\n","tokens = tk.tokenize(str)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_v75Nt4Mieq7","executionInfo":{"status":"ok","timestamp":1646457922680,"user_tz":-330,"elapsed":1394,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"e8029caf-0bb7-49a9-f4a7-6f128b30d745"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"]}]},{"cell_type":"markdown","source":["c. Tokenization using NLTK"],"metadata":{"id":"Oc1Vg8jKik2R"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\""],"metadata":{"id":"d9Nxpd0rikN9","executionInfo":{"status":"ok","timestamp":1646457996570,"user_tz":-330,"elapsed":404,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kzaq1Cn8i5P6","executionInfo":{"status":"ok","timestamp":1646458020965,"user_tz":-330,"elapsed":653,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"bf08ca1c-1cea-40e1-89f5-b0575830bf25"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Use tokenize method\n","print(word_tokenize(str))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfeXDNxTi084","executionInfo":{"status":"ok","timestamp":1646458023277,"user_tz":-330,"elapsed":12,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"b2933b66-2f6c-4cff-99fc-2130919c09ab"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"]}]},{"cell_type":"markdown","source":["d. Tokenization using the spaCy library"],"metadata":{"id":"fVUrdfRhi8FV"}},{"cell_type":"code","source":["import spacy\n","nlp = spacy.blank(\"en\")\n"],"metadata":{"id":"MFI6ctKbi9NZ","executionInfo":{"status":"ok","timestamp":1646458065566,"user_tz":-330,"elapsed":1120,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","# Create an instance of document;\n","# doc object is a container for a sequence of Token objects.\n","doc = nlp(str)\n","# Read the words; Print the words\n","#\n","words = [word.text for word in doc]\n","print(words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82xz4O2JjFJM","executionInfo":{"status":"ok","timestamp":1646458073150,"user_tz":-330,"elapsed":6,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"19b45403-6aec-41b3-e1ff-bbc907759df3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['I', 'love', 'to', 'study', 'Natural', 'Language', 'Processing', 'in', 'Python']\n"]}]},{"cell_type":"markdown","source":["e. Tokenization using Keras"],"metadata":{"id":"Rrj6ilVfjLI4"}},{"cell_type":"code","source":["#pip install keras\n","#pip install tensorflow\n","import keras\n","from keras.preprocessing.text import text_to_word_sequence\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","# tokenizing the text\n","tokens = text_to_word_sequence(str)\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDsPhNacjLmF","executionInfo":{"status":"ok","timestamp":1646458111756,"user_tz":-330,"elapsed":2838,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"f4cf55c5-a08b-4d85-a2f1-4426bb28ba5d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'love', 'to', 'study', 'natural', 'language', 'processing', 'in', 'python']\n"]}]},{"cell_type":"markdown","source":["f. Tokenization using Gensim"],"metadata":{"id":"IoB8QrpNjTQ9"}},{"cell_type":"code","source":["#pip install gensim\n","from gensim.utils import tokenize\n","# Create a string input\n","str = \"I love to study Natural Language Processing in Python\"\n","# tokenizing the text\n","list(tokenize(str))"],"metadata":{"id":"didzhemhjStm","executionInfo":{"status":"ok","timestamp":1646458134206,"user_tz":-330,"elapsed":633,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"848540c4-f124-4c00-a12e-240fda097906","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I',\n"," 'love',\n"," 'to',\n"," 'study',\n"," 'Natural',\n"," 'Language',\n"," 'Processing',\n"," 'in',\n"," 'Python']"]},"metadata":{},"execution_count":13}]}]}