{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"8_DIVYYA_NLP_PRACT_11.ipynb","provenance":[],"authorship_tag":"ABX9TyNo/sGpWSP9xMQNCGU8Z6sS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["a. Multiword Expressions in NLP."],"metadata":{"id":"ZaEjx7Uf1zIv"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zjqPlp0e1jpB","executionInfo":{"status":"ok","timestamp":1647838761990,"user_tz":420,"elapsed":683,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"c8ff188d-a63f-40fe-97f6-609d1fee09d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","['Good', 'cake', 'cost', 'Rs.1500\\\\kg', 'in', 'Mumbai', '.']\n","['Please', 'buy', 'me', 'one', 'of', 'them', '.']\n","['Thanks', '.']\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import MWETokenizer\n","from nltk import sent_tokenize, word_tokenize\n","s = '''Good cake cost Rs.1500\\kg in Mumbai. Please buy me one of them.\\n\\nThanks.'''\n","mwe = MWETokenizer([('New', 'York'), ('Hong', 'Kong')], separator='_')\n","for sent in sent_tokenize(s):\n"," print(mwe.tokenize(word_tokenize(sent)))"]},{"cell_type":"markdown","source":["b. Normalized Web Distance and Word Similarity."],"metadata":{"id":"FQ2vQr6-17Kn"}},{"cell_type":"code","source":["import numpy as np\n","import re \n","!pip install textdistance\n","import textdistance\n","# we will need scikit-learn>=0.21\n","import sklearn #pip install sklearn\n","from sklearn.cluster import AgglomerativeClustering\n","texts = ['Reliance supermarket', 'Reliance hypermarket', 'Reliance', 'Reliance', 'Reliance downtown', 'Relianc market','Mumbai', 'Mumbai Hyper', 'Mumbai dxb', 'mumbai airport', 'k.m trading', 'KM Trading', 'KM trade', 'K.M. Trading', 'KM.Trading']\n","def normalize(text):\n"," \"\"\" Keep only lower-cased text and numbers\"\"\"\n"," return re.sub('[^a-z0-9]+', ' ', text.lower())\n","def group_texts(texts, threshold=0.4): \n"," \"\"\" Replace each text with the representative of its cluster\"\"\"\n"," normalized_texts = np.array([normalize(text) for text in texts])\n"," distances = 1 - np.array([\n"," [textdistance.jaro_winkler(one, another) for one in normalized_texts] \n"," for another in normalized_texts\n"," ])\n"," clustering = AgglomerativeClustering(\n"," distance_threshold=threshold, # this parameter needs to be tuned carefully\n"," affinity=\"precomputed\", linkage=\"complete\", n_clusters=None\n"," ).fit(distances)\n"," centers = dict()\n"," for cluster_id in set(clustering.labels_):\n","  index = clustering.labels_ == cluster_id\n","  centrality = distances[:, index][index].sum(axis=1)\n","  centers[cluster_id] = normalized_texts[index][centrality.argmin()]\n"," return [centers[i] for i in clustering.labels_]\n","print(group_texts(texts))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7ytcHr916qn","executionInfo":{"status":"ok","timestamp":1647838808791,"user_tz":420,"elapsed":4144,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"d563e0b1-b409-4750-a765-72d2fe1e854e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textdistance\n","  Downloading textdistance-4.2.2-py3-none-any.whl (28 kB)\n","Installing collected packages: textdistance\n","Successfully installed textdistance-4.2.2\n","['reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'reliance', 'mumbai', 'mumbai', 'mumbai', 'mumbai', 'km trading', 'km trading', 'km trading', 'km trading', 'km trading']\n"]}]},{"cell_type":"markdown","source":["c. Word Sense Disambiguation."],"metadata":{"id":"PdAskykZ2PPX"}},{"cell_type":"code","source":["#Word Sense Disambiguation\n","import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","def get_first_sense(word, pos=None):\n"," if pos:\n","  synsets = wn.synsets(word,pos)\n"," else:\n","  synsets = wn.synsets(word)\n"," return synsets[0]\n","best_synset = get_first_sense('bank')\n","print ('%s: %s' % (best_synset.name, best_synset.definition))\n","best_synset = get_first_sense('set','n')\n","print ('%s: %s' % (best_synset.name, best_synset.definition))\n","best_synset = get_first_sense('set','v')\n","print ('%s: %s' % (best_synset.name, best_synset.definition))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0QSafQpL2Qm_","executionInfo":{"status":"ok","timestamp":1647838886203,"user_tz":420,"elapsed":3640,"user":{"displayName":"8_DIVYYA RAJPAL","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10408944483728759412"}},"outputId":"257332ae-d79f-4d10-94f6-9b2081a7fc64"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","<bound method Synset.name of Synset('bank.n.01')>: <bound method Synset.definition of Synset('bank.n.01')>\n","<bound method Synset.name of Synset('set.n.01')>: <bound method Synset.definition of Synset('set.n.01')>\n","<bound method Synset.name of Synset('put.v.01')>: <bound method Synset.definition of Synset('put.v.01')>\n"]}]}]}